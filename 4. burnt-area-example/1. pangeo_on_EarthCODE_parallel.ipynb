{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "395c42f3-5ca6-4e54-9a25-a1dfbca2df6b",
   "metadata": {},
   "source": [
    "---\n",
    "title: Pangeo X EarthCODE\n",
    "subtitle: D.03.10 HANDS-ON TRAINING - EarthCODE 101 Hands-On Workshop - Example showing how to access data on the EarthCODE Open Science Catalog and working with the Pangeo ecosystem on EDC\n",
    "authors:\n",
    "  - name: Deyan Samardzhiev\n",
    "    github: sunnydean\n",
    "    orcid: 0009-0003-3803-8522\n",
    "    affiliations:\n",
    "      - id: Lampata UK\n",
    "        institution: Lampata UK\n",
    "reviewers:\n",
    "  - name: Anne Fouilloux\n",
    "    orcid: 0000-0002-1784-2920\n",
    "    github: annefou\n",
    "    affiliations:\n",
    "      - id: Simula Research Laboratory\n",
    "        institution: Simula Research Laboratory\n",
    "        ror: 00vn06n10\n",
    "date: 2025-06-01\n",
    "thumbnail: https://raw.githubusercontent.com/ESA-EarthCODE/documentation/refs/heads/main/pages/public/img/EarthCODE_kv_transparent.png\n",
    "keywords: [\"earthcode\", \"pangeo\", \"stac\", \"xarray\", \"earth observation\", \"remote sensing\"]\n",
    "tags: [\"pangeo\"]\n",
    "releaseDate: 2025-06-01\n",
    "datePublished: 2025-06-01\n",
    "dateModified: 2025-06-01\n",
    "banner: ../static/PANGEO.png\n",
    "github: https://github.com/sunnydean/LPS25_Pangeo_x_EarthCODE_Workshop\n",
    "license: MIT\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b5c9be",
   "metadata": {},
   "source": [
    "## Table of Content\n",
    "```{contents}\n",
    ":depth: 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6edfbd-b4c2-4f56-a315-8355e53e52ac",
   "metadata": {},
   "source": [
    "## Context\n",
    "We will be using the [Pangeo](https://pangeo.io/) open-source software stack to demonstrate how to fetch EarthCODE published data and publically available Sentinel-2 data to generate burn severity maps for the assessment of the areas affected by wildfires.\n",
    "\n",
    "In this workshop, we will be using the [SeasFire Data Cube](https://opensciencedata.esa.int/products/seasfire-cube/collection) published to the EarthCODE Open Science Catalog.\n",
    "\n",
    "\n",
    "### Methodology approach\n",
    "* Analyse and find burnt areas using SeasFire Data Cube\n",
    "* Access Sentinel-2 L2A cloud optimised dataset through STAC\n",
    "* Compute the Normalised Burn Ratio (NBR) index to highlight burned areas\n",
    "* Classify burn severity\n",
    "\n",
    "### Highlights\n",
    "* Using OSC data\n",
    "* The NBR index uses near-infrared (NIR) and shortwave-infrared (SWIR) wavelengths.\n",
    "\n",
    "\n",
    "## Data\n",
    "We will use Sentinel-2 data accessed via [element84's STAC API](https://element84.com/earth-search/) endpoint and the [SeasFire Data Cube](https://opensciencedata.esa.int/products/seasfire-cube/collection) to find burned areas, inspect them in more detail and generate burn severity maps for the assessment of the areas affected by wildfires.\n",
    "\n",
    "\n",
    "\n",
    "#### Related publications\n",
    "* https://www.sciencedirect.com/science/article/pii/S1470160X22004708#f0035\n",
    "* https://github.com/yobimania/dea-notebooks/blob/e0ca59f437395f7c9becca74badcf8c49da6ee90/Fire%20Analysis%20Compiled%20Scripts%20(Gadi)/dNBR_full.py\n",
    "* *Alonso, Lazaro, Gans, Fabian, Karasante, Ilektra, Ahuja, Akanksha, Prapas, Ioannis, Kondylatos, Spyros, Papoutsis, Ioannis, Panagiotou, Eleannna, Michail, Dimitrios, Cremer, Felix, Weber, Ulrich, & Carvalhais, Nuno. (2022). SeasFire Cube: A Global Dataset for Seasonal Fire Modeling in the Earth System (0.4) [Data set]. Zenodo. @alonso-2024. The same dataset can also be downloaded from Zenodo: https://zenodo.org/records/13834057*\n",
    "* *https://registry.opendata.aws/sentinel-2-l2a-cogs/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11295d7-5aed-48bb-bacc-b493b6d990e8",
   "metadata": {},
   "source": [
    "### Import Packages\n",
    "\n",
    "As best practices dictate, we recommend that you install and import all the necessary libraries at the top of your Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22168f3-4e2e-4835-85b0-98fdb2d96d28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import xarray\n",
    "import rasterio\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "import hvplot.xarray\n",
    "import dask.distributed\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import shapely\n",
    "\n",
    "from pystac_client import Client as pystac_client\n",
    "from odc.stac import stac_load\n",
    "\n",
    "import os\n",
    "import xrlint.all as xrl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef32aef-38c6-4a2d-b6ff-876059065e85",
   "metadata": {},
   "source": [
    "# Create and startup your Dask Cluster\n",
    "\n",
    "Create dask cluster as described in the [dask 101 guide](../pangeo101/dask101.ipynb). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e7c494-ae16-4556-b6b0-17a3b0cae7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-26 12:54:06,721 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:44015' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('sub-64ef4d74e5724744d37c1ccb8c6ff8a2', 0, 0), ('truediv-16e0023acc00069c5f9e52e80f00bb2a', 0, 0)} (stimulus_id='handle-worker-cleanup-1758887646.7214787')\n",
      "2025-09-26 12:54:06,747 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:35997' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('truediv-89e28acc5eff1e125a61befbe27fecec', 0, 0)} (stimulus_id='handle-worker-cleanup-1758887646.7470853')\n"
     ]
    }
   ],
   "source": [
    "#create a local dask cluster on a local machine.\n",
    "\n",
    "from dask.distributed import Client\n",
    "client = Client()   \n",
    "client\n",
    "# client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94aed832-94cf-4431-8f56-f87d020b5760",
   "metadata": {},
   "source": [
    "# Load the Data\n",
    "\n",
    "Lazy load in data in Xarray as described in the [xarray 101 guide](../pangeo101/xarray101.ipynb). Note that the SeasFire data is +100GB and hosted remotely, we are just loading metadata, xarray will load only the chunks (dask data arrays) needed for our computations as discussed in the [dask 101 guide](../pangeo101/dask101.ipynb).\n",
    "\n",
    "The SeasFire Cube contains various variables for forecasting seasonal fires across the globe, we will specifically be using it to access historical wildfire data:\n",
    "\n",
    "*The SeasFire Cube is a scientific datacube for seasonal fire forecasting around the globe. It has been created for the SeasFire project, that adresses 'Earth System Deep Learning for Seasonal Fire Forecasting' and is funded by the European Space Agency (ESA)  in the context of ESA Future EO-1 Science for Society Call. It contains almost 20 years of data (2001-2021) in an 8-days time resolution and 0.25 degrees grid resolution. It has a diverse range of seasonal fire drivers. It expands from atmospheric and climatological ones to vegetation variables, socioeconomic and the target variables related to wildfires such as burned areas, fire radiative power, and wildfire-related CO2 emissions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9069e4-6606-4457-8c5b-704b5b941d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "http_url = \"https://s3.waw4-1.cloudferro.com/EarthCODE/OSCAssets/seasfire/seasfire_v0.4.zarr/\"\n",
    "\n",
    "ds = xarray.open_dataset(\n",
    "\thttp_url,\n",
    "\tengine='zarr',\n",
    "    chunks={},\n",
    "\tconsolidated=True\n",
    "\t# storage_options = {'token': 'anon'}\n",
    ")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a609134-e1d1-4d01-9b6a-e061d732c4e2",
   "metadata": {},
   "source": [
    "# Analyse Wildfires\n",
    "Our first task will be to find the date of the largest burnt area over the region we were previously exploring, so that we may analyse it in detail.\n",
    "\n",
    "One of the data variables from the SeasFire DataCube is the Burned Areas data from Global Wildfire Information System (GWIS). Each entry gives us the hectares (ha) of burnt area in that region. Note that our input resolution is 0.25 degrees (approx. 28x28 km per pixel), spanning 20 years (2001-2021) at an 8 days time resolution.\n",
    "\n",
    "First we lazy load just the GWIS dataset in a variable for convenience (note that again, this does not take up a large amount of memory since we're just working with the metadata at this point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fade082d-87d8-4c35-aac2-67b347dc67c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gwis = ds.gwis_ba\n",
    "gwis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36efba09-2d99-4137-a63a-e659e08882fa",
   "metadata": {},
   "source": [
    "# Spatial and Temporal Subsetting\n",
    "It's best practice to subset only the relevant data before doing computations, to reduce needed computations and data downloads. Especialyl when working with larger datasets. We will select an area of interest (aoi) as below using the polygon we previously downloaded from the xcube viewer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969a50d9-279b-4b61-b4a0-34381305d6cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Spatial Slices\n",
    "epsg = 4326 # our data's projection\n",
    "\n",
    "gdf = gpd.read_file(\"../aoi/feature.geojson\")\n",
    "gdf = gdf.set_crs(epsg=epsg)\n",
    "gdf.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b5ff45-a257-4a7f-be39-22d0079fe7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "min_lon, min_lat, max_lon, max_lat = gdf.total_bounds\n",
    "\n",
    "# find the nearest points on our grid\n",
    "lat_start = gwis.latitude.sel(latitude=max_lat, method=\"nearest\").item()\n",
    "lat_stop  = gwis.latitude.sel(latitude=min_lat, method=\"nearest\").item()\n",
    "lon_start = gwis.longitude.sel(longitude=min_lon, method=\"nearest\").item()\n",
    "lon_stop  = gwis.longitude.sel(longitude=max_lon, method=\"nearest\").item()\n",
    "\n",
    "lat_slice = slice(lat_start, lat_stop)\n",
    "lon_slice = slice(lon_start, lon_stop)\n",
    "\n",
    "lon_start, lat_start, lon_stop, lat_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb85dbf8-b984-4c2c-b43f-2ec5137b0c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox=[lon_stop , lat_stop , lon_start, lat_start]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2782e3-163e-40a0-8179-8b59932da82c",
   "metadata": {},
   "source": [
    "We additionally will subest our data temporally to fetch only more recent periods from 2018 - 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795173c8-3e16-46e8-8956-3b3addb986a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_slice = slice('2018-01-01','2021-01-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1597d0-5918-4444-bff9-94e37c47d501",
   "metadata": {},
   "source": [
    "We now simply subset the data using the slices we created. Note how the data of our computation has reduced significantly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d33b7a-61a3-468a-9759-9101d627d1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatio temporal subset\n",
    "gwis_aoi = gwis.sel(time=time_slice, latitude=lat_slice,longitude=lon_slice)\n",
    "gwis_aoi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfa6bd0-e9c8-4203-989c-e71885c3b89c",
   "metadata": {},
   "source": [
    "# Finding the date with the most burnt area\n",
    "\n",
    "To find the date with the most burnt area we first do a spatial sum over our data for each period; this will return an dataarray with the total amount of ha of burnt area for each day. We will then fetch the date with the max burned area using idmax.\n",
    "\n",
    "*Note idmax is equivalent to doing ds.sel(time=ds.argmax.item(dim='time')) where argmax returns the index of the max item*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95f71db-2fad-4be2-8e6a-5dafc48ee6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# date where the sum of burnt area is the highest\n",
    "date_max_fire = gwis_aoi.sum(dim={'latitude','longitude'}).idxmax(dim='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6bfd75-d689-4ba1-a606-88b51f8acafe",
   "metadata": {},
   "source": [
    "By subsetting only to the relevant data, we have reduced our dataset from 3.7 GB to a very small managable chunk of relevant data. Note that up to this point, we have not downloaded any data or done any computations over it. We (via the Dask Client) have just built up in the background the dask task graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7e9760-cb98-4369-a0d7-320fe71aa621",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_max_fire.data.visualize(optimize_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd6f3ee-14f4-4216-9112-0393b9065dbd",
   "metadata": {},
   "source": [
    "## Let's start computing!\n",
    "\n",
    "Let's get our date by forcing dask to execute the above task graph using .compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d792c80-767a-4584-86fe-fab9b8c84b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_max_fire = date_max_fire.compute()\n",
    "date_max_fire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5708d6fa-1820-4fd7-acca-9f5a65d078c2",
   "metadata": {},
   "source": [
    "# Exploring the Data\n",
    "Great, we've found the date with the most burnt areas, now let's explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6348a6c-20a1-482b-bf3d-9e7dc6ae29f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "biggest_fire_aoi = gwis_aoi.sel(time=date_max_fire)\n",
    "biggest_fire_aoi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb13e92-0c81-4fd5-8986-671d9363f1af",
   "metadata": {},
   "source": [
    "Plot the forest fire areas to get an idea about our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597bbbfa-c049-4043-ad3b-5ce28c48c353",
   "metadata": {},
   "source": [
    "Let's plot our data to explore it a bit further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b95b98-5dcb-449c-bb80-d9abc4648ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "biggest_fire_aoi.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52784474-6db5-4e9d-a080-6fd8bee5bf0a",
   "metadata": {},
   "source": [
    "The image above does not really give us a detailed view of which areas have been burned and to what degree, as the resolution of each pixel is approx 28km. To get some more context of our data we can plot it on a base map.\n",
    "\n",
    "In the Pangeo stack there are visualization tools that can help us easily plot data in a more interactive way, with a simple interface - such as hvplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e2578e-c9e7-4a8f-a06c-cf5cdd5656b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot it interactively, with some context\n",
    "biggest_fire_aoi.hvplot(\n",
    "    x='longitude',\n",
    "    y='latitude',\n",
    "    cmap='viridis',\n",
    "    colorbar=True,\n",
    "    frame_width=600,\n",
    "    frame_height=400,\n",
    "    geo=True,\n",
    "    tiles='OSM',\n",
    "    alpha=0.5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbccea0a-6f66-4eb3-8d1e-a012db99ce9b",
   "metadata": {},
   "source": [
    "# Analysing in Detail\n",
    "Now that we have an idea of a potential wildfire event, we can explore in a bit more detail to see what areas where affected most. \n",
    "\n",
    "We will use higher resolution sentinel-2-l2a data to compute the Normalised Burn Ratio (NBR) index to highlight burned areas. The NBR index uses near-infrared (NIR) and shortwave-infrared (SWIR) wavelengths. We will use a STAC API [element84's STAC API](https://element84.com/earth-search/) endpoint to search for the data and load only the relevant, cloud-free data.\n",
    "\n",
    "To estimate the amount of burnt area we will compute the difference between the NBR from period before the fire date and the NBR from the period after. The first step is to select the week before and the week after the wildfire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217c0303-e1e5-42a3-939c-1efebf237b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_date_t = pd.to_datetime(date_max_fire.values.item()) # get the date of the forest fire and a the dates before and after it\n",
    "week_before = (fire_date_t - timedelta(days=7))\n",
    "week_after = (fire_date_t + timedelta(days=7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcea67e-ae5e-4764-bbdf-58541a72688b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(week_before.date(), \"---\" , week_after.date())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d26d983-5c9f-4070-81af-fdbb64aabcd6",
   "metadata": {},
   "source": [
    "## Fetching the data\n",
    "We will now fetch the data using pystac to search through the earth-search.aws.element84.com/v1 STAC API endpoint. As mentioned in the [STAC and Data Access guide](../earthcode101/stac_and_data_access.ipynb), the STAC API provides us with endpoints to query STAC collections.\n",
    "\n",
    "There is an ecosystem of libraries that implement STAC standards (pystac, odc-stac, stackstac, etc..) that allow us to analyse, load and use data described in STAC. We will briefly explore in the following cells below.\n",
    "\n",
    "As a first step, we will open a catalog with the pystac library: https://earth-search.aws.element84.com/v1 This catalog contains STAC collections of various datasets (such as sentinel-1 and sentinel-2 data) in cloud optimised formats (COGs in this case) - see https://earth-search.aws.element84.com. for more details. With these formats we are able to use the assets using cloud-native access patterns (see https://guide.cloudnativegeo.org/cookbooks/ for more details).\n",
    "\n",
    "Note that a STAC endpoint like the one below, only returns STAC items and not the actual data. The STAC items returned have Assets defined which link to the actual data (and provide metadata)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e746ce-8b40-4299-b06e-173ff0743e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = pystac_client.open(\"https://earth-search.aws.element84.com/v1\")\n",
    "chunk={} # <-- use dask\n",
    "res=10 # 10m resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc43cc3-7fb5-4c7e-8341-6009602e6547",
   "metadata": {},
   "source": [
    "In the next step we will do two things:\n",
    "\n",
    "**1.** Search for multiple cloudless sentinel-2 satellite images within a month of our pre-fire (week_before) date. The STAC API allows us to do a couple of things in a simple API call:\n",
    "- We can define an arrea of interest (bbox) and search for items that cover this region\n",
    "- We can subset for the time of interest as well (datetime)\n",
    "- We can define custom querries over implemented STAC extensions. For example, the Sentinel STAC collection we are querrying implements the eo STAC extension, and defines cloud_cover - this allows us to search for quality assets with minimal cloudy pixels\n",
    "\n",
    "This will return the relevant STAC items - which contain assets that point to our data of interest. We now need to load this data into a dataarray to start analysing it.\n",
    "\n",
    "**2.** Note that in the above step, we only have items that point to some data - this data can be tiff, zarr, netcdf, COG, or other SpatioTemporal asset data. In our case, the element84 endpoint points to the data collection of  *https://registry.opendata.aws/sentinel-2-l2a-cogs/* - the format of our data is cloud-optimized GeoTiff. A cloud-friendly format such as this enables us cloud-native access patterns, such as easily fetching only our area of interest (as opposed to several tiff files and manually subsetting them after downloading).\n",
    "\n",
    "Libraries such as odc-stac integrate with STAC standards and allow us to load data as well as leveraging the cloud-optimised formats. For example, in the cell below we define how we want to transform/load our data by:\n",
    "- Passing the STAC item (or multiple items) we want to load\n",
    "- defining a particular chunk size (the passed {} asks for the data to be automatically chunked as it is originally);\n",
    "- We can request only the spectral bands of interest, in this case nir and swir22, to reduce the amount of data that we fetch.\n",
    "- We can define a resolution to retrieve the data at, note that this will also resample automatically. For example the nir band data has a 10m resolution, but the swir22 - 20m resolution.\n",
    "- There are multiple other options as well, such as defining in which projection we want our data in. More information can be found at: https://odc-stac.readthedocs.io/en/latest/_api/odc.stac.load.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d12f199-1c82-4253-8517-60b83de371f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# STAC search for relevant items\n",
    "week_before_start = (week_before - timedelta(days=30))\n",
    "time_range_start = str(week_before_start.date()) + \"/\" + str(week_before.date())\n",
    "\n",
    "query1 = catalog.search(\n",
    "    collections=[\"sentinel-2-l2a\"], datetime=time_range_start, limit=100,\n",
    "    bbox=bbox, query={\"eo:cloud_cover\": {\"lt\": 20}}\n",
    ")\n",
    "\n",
    "items = list(query1.items())\n",
    "print(f\"Found: {len(items):d} datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e539a52e-ba61-46d7-8c19-2f54e443b5c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot all the STAC assets\n",
    "poly_pre = gpd.GeoSeries([shapely.Polygon(item.geometry['coordinates'][0]) for item in items], name='geometry', crs='epsg:4236')\n",
    "poly_pre.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08b52ea-30cb-41c1-9236-cf35f6e94f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefire_ds = stac_load(\n",
    "    items,\n",
    "    bands=(\"nir\", \"swir22\"),\n",
    "    chunks=chunk,  # <-- use Dask\n",
    "    resolution=res,\n",
    "    crs=\"EPSG:32629\",\n",
    "    groupby=\"datetime\",\n",
    "    bbox=bbox,\n",
    ")\n",
    "prefire_ds = prefire_ds.mean(dim=\"time\")\n",
    "prefire_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45427530-45c3-4bd3-92f6-95361527d06a",
   "metadata": {},
   "source": [
    "We now do the same for the month after.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe7055b-5782-4af6-9c10-94819659ffe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "week_after_end = (week_after + timedelta(days=30))\n",
    "time_range_end = str(week_after.date()) + \"/\" + str(week_after_end.date())\n",
    "\n",
    "query2 = catalog.search(\n",
    "    collections=[\"sentinel-2-l2a\"], datetime=time_range_end, limit=100,\n",
    "    bbox=bbox, query={\"eo:cloud_cover\": {\"lt\": 20}}\n",
    ")\n",
    "\n",
    "items = list(query2.items())\n",
    "print(f\"Found: {len(items):d} datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705940a4-a16d-4474-91e3-52f312153e5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "poly_post = gpd.GeoSeries([shapely.Polygon(item.geometry['coordinates'][0]) for item in items], name='geometry', crs='epsg:4236')\n",
    "poly_post.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61420ac4-0679-4b9a-b41e-d973f4f5f3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "postfire_ds = stac_load(\n",
    "    items,\n",
    "    bands=(\"nir\", \"swir22\"),\n",
    "    chunks=chunk,  # <-- use Dask\n",
    "    resolution=res,\n",
    "    crs=\"EPSG:32629\",\n",
    "    groupby=\"datetime\",\n",
    "    bbox=bbox,\n",
    ")\n",
    "postfire_ds = postfire_ds.mean(dim=\"time\")\n",
    "postfire_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321c9abf-1ef5-4c9c-9dd2-23a9fcbda893",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_poly_pre = poly_pre[[poly_pre.to_crs(epsg=3035).area.argmax()]]\n",
    "max_poly_post = poly_post[[poly_post.to_crs(epsg=3035).area.argmax()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97e23fe-0d7d-4e5c-a37b-d62433e6a799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note we're reprojecting to calculate area, as Geometry is in a geographic CRS. Results from 'area' are incorrect since geopandas doesn't calc spherical geometry!\n",
    "poly_pre.area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c8b12c-c49e-43ef-8b81-f26cd7467327",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m = max_poly_pre.explore( )\n",
    "m = max_poly_post.explore(m=m, color='r')\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56f174f-0c1b-4a62-aa1e-188bba8f8b3c",
   "metadata": {},
   "source": [
    "> **Note:** The above STAC API query might return assets that intersect the region but do not fully cover it. I.e. if we fetched only one asset for each period with the above query they could potentially point to different regions (as there would be data missing). Although not relevant in this example, as all assets from both pre and after overlap, this will be a problem if not checked (as you'd be analysing different regions). In this example we visually inspect the overlap between assets and further take the mean of all the assets to get better quality pixels across time. Rerun the same example with cloud_cover = 0.2 to see where you might get some problem code!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace3d425-8a22-415c-858d-c500bb2750f7",
   "metadata": {},
   "source": [
    "# Calculating the NBR index\n",
    "\n",
    "In the next step we will calculate our index, using simple band math as explained in the [xarray 101 guide](../pangeo101/xarray101.ipynb). We first define a function that calculates our index given a dataset with nir/swir22 band data and then add the additional variable to our pre-fire/post-fire datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb455db-6aff-411c-b60e-bcc364604954",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = 'NBR'\n",
    "\n",
    "# Normalised Burn Ratio, Lopez Garcia 1991\n",
    "def calc_nbr(ds):\n",
    "    return (ds.nir - ds.swir22) / (ds.nir + ds.swir22)\n",
    "\n",
    "index_dict = {'NBR': calc_nbr}\n",
    "index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4362690-e4ab-4074-9095-672edb85787a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prefire - calls the calc_nbr function with our dataset as input to create a new NBR index (dict) and assigns it as the NBR index \n",
    "# note that we have to apply scaling (1000) and offset (0) to our band data - as defined in the dataset collection\n",
    "prefire_ds[index_name] = index_dict[index_name](prefire_ds / 10000.0) \n",
    "\n",
    "# postfire - calls the calc_nbr function with our dataset as input to create a new NBR index (dict) and assigns it as the NBR index \n",
    "# note that we have to apply scaling (1000) and offset (0) to our band data - as defined in the dataset collection\n",
    "postfire_ds[index_name] = index_dict[index_name](postfire_ds / 10000.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0660ff36-4220-4d6c-ab81-b007c96b65d0",
   "metadata": {},
   "source": [
    "Now that we have the indecies calculated we can calculate the difference in burnt area between the two periods to analyse which regions have been burnt.\n",
    "\n",
    "Note that at this point we haven't actually loaded the data or done the calculations, we've just defined a task graph for dask to execute. When we call persist() the graph up to that point will be executed and the data saved for in the distributed memory of our workers. We do this at this stage to avoid fetching the data multiple times in future computations.\n",
    "\n",
    "We then save our result (which is a dataarray) in a new dataset: dnbr_dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f594ea4-3091-48fd-9a70-8f26fdd94054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate delta NBR\n",
    "prefire_burnratio = prefire_ds.NBR.persist() # <--- load and keep data into your workers\n",
    "postfire_burnratio = postfire_ds.NBR.persist() # <--- load and keep data into your workers\n",
    "\n",
    "delta_NBR = prefire_burnratio - postfire_burnratio\n",
    "\n",
    "dnbr_dataset = delta_NBR.to_dataset(name='delta_NBR').persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7e1aca-5885-4f86-8d50-951ed9b2182f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnbr_dataset\n",
    "delta_NBR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9a460f-9cc2-412c-919a-31eb6da5dc39",
   "metadata": {},
   "source": [
    "# Plotting and visualization\n",
    "\n",
    "We will now plot our data from before, after and the delta of our wildfire event to further analyse. Note that, this will trigger the execution of our dask task graph. The Pangeo stack offers many tools for visualization! In the example below we will showcase plotting with Cartopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16515881-6a96-48f3-968c-7763b48aed8a",
   "metadata": {},
   "source": [
    "## Plotting Before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5358844d-1afc-4d5d-aa68-eb07c39b029c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=[7, 10])\n",
    "\n",
    "# We're using cartopy and are plotting in PlateCarree projection \n",
    "# (see documentation on cartopy)\n",
    "ax = plt.subplot(1, 1, 1, projection=ccrs.PlateCarree())\n",
    "ax.coastlines(resolution='10m')\n",
    "ax.gridlines(draw_labels=True)\n",
    "\n",
    "# We need to project our data to the new Orthographic projection and for this we use `transform`.\n",
    "# we set the original data projection in transform (here Mercator)\n",
    "prefire_burnratio.plot(ax=ax, transform=ccrs.epsg(prefire_burnratio.spatial_ref.values), cmap='RdBu_r',\n",
    "                       cbar_kwargs={'orientation':'horizontal','shrink':0.95})\n",
    "\n",
    "# One way to customize your title\n",
    "plt.title(\"Pre-Fire: \" + time_range_start, fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80ff7f6-3920-4f9e-a5cb-2b940e81720b",
   "metadata": {},
   "source": [
    "## Plotting After"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902ddf95-672f-432f-b1b2-047b4f4a54ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=[7, 9])\n",
    "\n",
    "# We're using cartopy and are plotting in PlateCarree projection \n",
    "# (see documentation on cartopy)\n",
    "ax = plt.subplot(1, 1, 1, projection=ccrs.PlateCarree())\n",
    "ax.coastlines(resolution='10m')\n",
    "ax.gridlines(draw_labels=True)\n",
    "\n",
    "# We need to project our data to the new Orthographic projection and for this we use `transform`.\n",
    "# we set the original data projection in transform (here Mercator)\n",
    "postfire_burnratio.plot(ax=ax, transform=ccrs.epsg(postfire_burnratio.spatial_ref.values), cmap='RdBu_r',\n",
    "                        cbar_kwargs={'orientation':'horizontal','shrink':0.95})\n",
    "\n",
    "# One way to customize your title\n",
    "plt.title(\"Post-Fire: \" + time_range_end, fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e439f487-108e-46c2-87aa-24d9d7b26cb4",
   "metadata": {},
   "source": [
    "## Plotting Delta\n",
    "\n",
    "The plot below highligths the burnt regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14caa7b7-7026-4039-8da4-02a1394d40ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=[7, 10])\n",
    "\n",
    "# We're using cartopy and are plotting in PlateCarree projection \n",
    "# (see documentation on cartopy)\n",
    "ax = plt.subplot(1, 1, 1, projection=ccrs.PlateCarree())\n",
    "ax.coastlines(resolution='10m')\n",
    "ax.gridlines(draw_labels=True)\n",
    "\n",
    "# We need to project our data to the new Orthographic projection and for this we use `transform`.\n",
    "# we set the original data projection in transform (here Mercator)\n",
    "dnbr_dataset.delta_NBR.plot(ax=ax, transform=ccrs.epsg(dnbr_dataset.delta_NBR.spatial_ref.values), cmap='RdBu_r',\n",
    "                            cbar_kwargs={'orientation':'horizontal','shrink':0.95})\n",
    "\n",
    "# One way to customize your title\n",
    "plt.title( \"Delta NBR\", fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20ce2a3-d72c-4d98-886a-6765a7cde16b",
   "metadata": {},
   "source": [
    "# Analysing Burnt Areas \n",
    "\n",
    "Great, now that we've calculated the delta NBR (dNBR), we can estimate the amount of area that was burned. First we need to classify which pixels are actually burned and then calculate the area that was burned.\n",
    "\n",
    "To classify burned area we can use the dNBR reference ratio ratios described in: [https://un-spider.org/advisory-support/recommended-practices/recommended-practice-burn-severity/in-detail/normalized-burn-ratio](https://un-spider.org/advisory-support/recommended-practices/recommended-practice-burn-severity/in-detail/normalized-burn-ratio) - and estimate the moderate to severe burnt areas. **i.e. the areas with a dNBR of above 0.440**\n",
    "\n",
    "![img](https://un-spider.org/sites/default/files/table+legend.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceebd22-b673-4bee-9489-d7f22904fc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "BURN_THRESH = 0.440\n",
    "burn_mask = dnbr_dataset.delta_NBR > BURN_THRESH           # True/False mask, same shape as raster\n",
    "burn_mask.plot()\n",
    "\n",
    "# save the burn_mask to our dataset\n",
    "dnbr_dataset['burned_ha_mask'] = burn_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afc4805-9453-4884-96e0-f6e3ed76a80e",
   "metadata": {},
   "source": [
    "# Simple Validation\n",
    "\n",
    "Now with the above a data mask where all pixels classified as burnt are 1 and the non burnt ones are 0. To calculate burnt area, we simply sum all pixels and multiply them by the area which a pixel covers (i.e. the resolution, which in our case is 10m/p).\n",
    "\n",
    "At the same step we will convert our metric to hectares (ha) so that we can compare and validate our findings against the original GWIS data.\n",
    "\n",
    "*Note that in an actual study, the ground truth data and validation would involve field studies and more precise validation, but we simplify the example for the training.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e1af97-4521-471c-99fb-6e934e2a3300",
   "metadata": {},
   "outputs": [],
   "source": [
    "dx, dy = dnbr_dataset.delta_NBR.rio.resolution()\n",
    "pixel_area_ha = abs(dx * dy) / 1e4       # 10m × 10m = 0.01 ha\n",
    "\n",
    "pixels_burned   = burn_mask.sum().compute().item()   # integer number of burned pixels\n",
    "burned_area_ha  = pixels_burned * pixel_area_ha\n",
    "\n",
    "print(f\"Burned area   : {burned_area_ha:,.2f} ha\")\n",
    "print(f\"Actual Burned Area : {biggest_fire_aoi.sum().compute():,.2f}, ha\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf6f1f4-4fa0-49b1-8394-001324a7fe6d",
   "metadata": {},
   "source": [
    "There's only a difference of about 300 ha, which, given our simple analysis, is actually quite accurate!\n",
    "\n",
    "*note this is based on the geojson feature in the github repo* with the following result:\n",
    "- Burned area   : 1,942.72 ha\n",
    "- Actual Burned Area : 2,214.27, ha\n",
    "\n",
    "\n",
    "\n",
    "## Plotting and Analysing Final Results\n",
    "\n",
    "We can visually inspect our plot to see if our data follows the same trends as our ground truth GWIS dataset.\n",
    "\n",
    "To achieve this though, we need to reproject our data, since the sentinel-2 data we are using is in a different projection with a higher resolution (10m/pixel) and the SeasFire GWIS ground truth data has a resolution of 0.25 degrees (approx 28km x 28 km).\n",
    "\n",
    "We will do this using the *rioxarray library*, which offers convenient methods such as reproject_match, which takes in as an argument a dataarray to which to match the original dataset. It essentially reprojects and matches the grid of a datarray to match the grid/projection of another and upsamples (or downsamples) the data to match resolutions, as well as subsetting the data to match.\n",
    "\n",
    "Learn more about how it works at: https://corteva.github.io/rioxarray/stable/examples/reproject_match.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792f1959-938a-41d9-8673-7f030608e7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure that our dataarray has metadata about its projection - this is used by rioxarray.\n",
    "biggest_fire_aoi = biggest_fire_aoi.rio.write_crs(ds.rio.crs)\n",
    "\n",
    "biggest_fire_aoi_reprojected = biggest_fire_aoi.rio.reproject_match(dnbr_dataset.burned_ha_mask)\n",
    "\n",
    "\n",
    "burned_ha_mask_plot = dnbr_dataset.burned_ha_mask.hvplot(\n",
    "    width=700,\n",
    "    height=700,\n",
    "    title='dNBR (10 m) with GWIS overlay',\n",
    "    alpha=1.0\n",
    ")\n",
    "\n",
    "# Plot the reprojected coarse dataset as transparent overlay\n",
    "gwis_plot = biggest_fire_aoi_reprojected.hvplot(\n",
    "    cmap='Reds',\n",
    "    alpha=0.3,\n",
    "    clim=(0, biggest_fire_aoi.max().compute().item())\n",
    ")\n",
    "\n",
    "# Combine them interactively\n",
    "combined_plot = burned_ha_mask_plot * gwis_plot\n",
    "\n",
    "combined_plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea004b6-30dc-4ffa-a20b-32f1abd6deed",
   "metadata": {},
   "source": [
    "Great! Our plot generally follows the same trends as our burn analysis, the north-eastern and north-western regions seemed to be mainly burnt with two separately affected regions.\n",
    "\n",
    "Our plot and values are quite a bit off though if you try to sum them - why is that? \n",
    "\n",
    "**Homework: Fix it and add it as a variable!**\n",
    "\n",
    "*hint: ha*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a2508b-51de-40b7-9268-55352aebb8b3",
   "metadata": {},
   "source": [
    "# Saving Your Work\n",
    "\n",
    "Now let's save our work! For intraoperability, we will save our data as a valid data cube in Zarr.\n",
    "\n",
    "Keep in mind that EarthCODE provides different tooling that makes it easy to publish our data to the wider EO community on the EarthCODE Open Science Catalog (such as deep-code for publishing data cubes, we will see in the [**publishing guide**](../publishing-to-earthcode/deep-code/deep-code/publish-pangeo.ipynb)) by following common standards and using common file formats we ensure that there will be a tool to help us!\n",
    "\n",
    "\n",
    "## Linting\n",
    "\n",
    "There are also tools to ensure the quality of our data/metadata. Two very useful tools are `xrlint` which check our datarray against common expected standards and advise corrections (such as missing attributes which should be filled in). It's basically a linter for xarray - xarray + linter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af87fe44-6081-4600-8a49-59d6596596a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "linter = xrl.new_linter(\"recommended\")\n",
    "linter.validate(dnbr_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18400a43-7aec-480f-ad15-17cbb7eddb53",
   "metadata": {},
   "source": [
    "### Add metadata descriptions to our data\n",
    "\n",
    "As we see, there is quite a few attributes missing from our data cube. As a best practices, it's recommended to have it very well described, to ensure FAIR-ness and, specifically, intraoperability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27975471-0a3b-4a8d-ae9d-0cbe281fe265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign dataset-level attributes\n",
    "dnbr_dataset.attrs.update({\n",
    "    'title': 'Delta NBR and Burned Area Mask Dataset',\n",
    "    'history': 'Created by reprojecting and aligning datasets for fire severity analysis',\n",
    "    'Conventions': 'CF-1.7'\n",
    "})\n",
    "\n",
    "\n",
    "# Assign variable-level attributes for delta_NBR\n",
    "dnbr_dataset.delta_NBR.attrs.update({\n",
    "    'institution': 'Lampata',\n",
    "    'source': 'Sentinel-2 imagery; processed with open-source dNBR code, element84...',\n",
    "    'references': 'https://registry.opendata.aws/sentinel-2-l2a-cogs/',\n",
    "    'comment': 'dNBR values represent change in vegetation severity post-fire',\n",
    "    'standard_name': 'difference_normalized_burn_ratio',\n",
    "    'long_name': 'Differenced Normalized Burn Ratio (dNBR)',\n",
    "    'units': 'm'\n",
    "})\n",
    "\n",
    "# Example for burned_ha_mask data variable\n",
    "dnbr_dataset.burned_ha_mask.attrs.update({\n",
    "    'standard_name': 'burned_area_mask',\n",
    "    'long_name': 'Burned Area Mask in Hectares',\n",
    "    'units': 'hectares',\n",
    "    'institution': 'Your Institution Name',\n",
    "    'source': 'Derived from wildfire impact analysis',\n",
    "    'references': 'https://example.com/ref',\n",
    "    'comment': 'Burned area mask showing presence of burned areas'\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8d4f26-6171-4297-9d33-495628f84f7b",
   "metadata": {},
   "source": [
    "### Why is this important?\n",
    "\n",
    "By ensuring your data follows common standards and FAIR principles, firstly and most importantly you enable make it usable by others and therefore increase its impact!\n",
    "\n",
    "You also enable libraries that implement the common standards you follow to use your data. For example, for our dataset above, if we do not apply the corrections (for time specifically) any future applications using xcube won't be able to open up our dataset. Applications or users trying to understand how we derived our data will not have information, and etc...\n",
    "\n",
    "The tools and standards help us along on the way to FAIR-ness!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a6396c-f8f7-4500-9b75-1f6ea8b1b134",
   "metadata": {},
   "source": [
    "## Saving and Chunking\n",
    "\n",
    "To make our data easier to use for future users we will chunk it the recommended chunk sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f2dd0c-4cca-4aad-8dc9-d1f6161670dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnbr_dataset = dnbr_dataset.chunk({\"y\": 1000, \"x\": 1000}).load()\n",
    "print(type(dnbr_dataset.burned_ha_mask.data)) # check data format "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5c562c-f7b5-4b5b-b396-44f011200860",
   "metadata": {},
   "source": [
    "Finally we create our local zarr store with .to_zarr. This will save the dataset locally.\n",
    "\n",
    "*Note: You can easily store it on cloud storge such as s3 with some slight edits to the code below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203e699c-3b6b-480f-903a-7d957dfdd062",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_at_folder = '../wildfires'\n",
    "if not os.path.exists(save_at_folder):\n",
    "    os.makedirs(save_at_folder)\n",
    "\n",
    "# Define the output path within your notebook folder\n",
    "output_path = os.path.join(save_at_folder, \"dnbr_dataset.zarr\")\n",
    "\n",
    "# save\n",
    "dnbr_dataset.to_zarr(output_path, mode=\"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb994c2-92d5-4119-92b0-302976c3270b",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Congratulations! You've gotten to the end of this notebook. We explored a simple workflow with the [Pangeo](https://pangeo.io/) open-source software stack, demonstrated how to fetch and access EarthCODE published data and publically available Sentinel-2 data to generate burn severity maps for the assessment of the areas affected by wildfires. Finally, we saved our work and learned about why FAIRness matters.\n",
    "\n",
    "As next steps we recommend reading through the documentation pages of each of the libraries, and going through the example in your own time in more detail! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1862158c-e8ac-406f-a629-330399a5953d",
   "metadata": {},
   "source": [
    "# Links and Refernces\n",
    "\n",
    "* https://earthcode.esa.int/\n",
    "* https://opensciencedata.esa.int/\n",
    "* https://www.sciencedirect.com/science/article/pii/S1470160X22004708#f0035\n",
    "* https://github.com/yobimania/dea-notebooks/blob/e0ca59f437395f7c9becca74badcf8c49da6ee90/Fire%20Analysis%20Compiled%20Scripts%20(Gadi)/dNBR_full.py\n",
    "* *Alonso, Lazaro, Gans, Fabian, Karasante, Ilektra, Ahuja, Akanksha, Prapas, Ioannis, Kondylatos, Spyros, Papoutsis, Ioannis, Panagiotou, Eleannna, Michail, Dimitrios, Cremer, Felix, Weber, Ulrich, & Carvalhais, Nuno. (2022). SeasFire Cube: A Global Dataset for Seasonal Fire Modeling in the Earth System (0.4) [Data set]. Zenodo. @alonso-2024. The same dataset can also be downloaded from Zenodo: https://zenodo.org/records/13834057*\n",
    "* https://opensciencedata.esa.int/products/seasfire-cube/collection)\n",
    "* https://un-spider.org/advisory-support/recommended-practices/recommended-practice-burn-severity/in-detail/normalized-burn-ratio](https://un-spider.org/advisory-support/recommended-practices/recommended-practice-burn-severity/in-detail/normalized-burn-ratio\n",
    "* *https://registry.opendata.aws/sentinel-2-l2a-cogs/*\n",
    "* element84's STAC API](https://element84.com/earth-search/)\n",
    "* https://github.com/bcdev/xrlint/tree/main/docs\n",
    "* https://xcube.readthedocs.io/en/latest/\n",
    "* https://docs.dask.org/en/stable/\n",
    "* https://docs.xarray.dev/en/stable\n",
    "* https://zarr.readthedocs.io/en/stable/\n",
    "* https://corteva.github.io/rioxarray/stable/examples/reproject_match.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c9086a-aa88-4667-986f-4a32589bdeef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:micromamba-pangeo_earthcode] *",
   "language": "python",
   "name": "conda-env-micromamba-pangeo_earthcode-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
