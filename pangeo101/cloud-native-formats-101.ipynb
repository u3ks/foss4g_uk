{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a83d8ea-41fa-4128-a879-1f20e24912b0",
   "metadata": {},
   "source": [
    "---\n",
    "title: Cloud Native Formats 101\n",
    "subtitle: D.03.10 HANDS-ON TRAINING - EarthCODE 101 Hands-On Workshop - Example showing how to access data on the EarthCODE Open Science Catalog and working with the Pangeo ecosystem on EDC\n",
    "authors:\n",
    "  - name: Deyan Samardzhiev\n",
    "    github: sunnydean\n",
    "    orcid: 0009-0003-3803-8522\n",
    "    affiliations:\n",
    "      - id: Lampata UK\n",
    "        institution: Lampata UK\n",
    "reviewers:\n",
    "  - name: Anne Fouilloux\n",
    "    orcid: 0000-0002-1784-2920\n",
    "    github: annefou\n",
    "    affiliations:\n",
    "      - id: Simula Research Laboratory\n",
    "        institution: Simula Research Laboratory\n",
    "        ror: 00vn06n10\n",
    "date: 2025-06-01\n",
    "thumbnail: https://raw.githubusercontent.com/ESA-EarthCODE/documentation/refs/heads/main/pages/public/img/EarthCODE_kv_transparent.png\n",
    "keywords: [\"earthcode\", \"pangeo\", \"stac\", \"xarray\", \"earth observation\", \"remote sensing\"]\n",
    "tags: [\"pangeo\"]\n",
    "releaseDate: 2025-06-01\n",
    "datePublished: 2025-06-01\n",
    "dateModified: 2025-06-01\n",
    "banner: ../static/PANGEO.png\n",
    "github: https://github.com/sunnydean/LPS25_Pangeo_x_EarthCODE_Workshop\n",
    "license: MIT\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d449b0-c3f6-4c03-87aa-4722a8305638",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd472037-38e4-4333-9dbe-8a00498106f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pystac.extensions.storage import StorageExtension\n",
    "from datetime import datetime\n",
    "from pystac_client import Client as pystac_client\n",
    "from odc.stac import configure_rio, stac_load\n",
    "\n",
    "import xarray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef74b7d2-395a-4a54-a9b2-a11a6c5d0ecd",
   "metadata": {},
   "source": [
    "## Context\n",
    "\n",
    "When dealing with large data files or collections, it's often impossible to load all the data you want to analyze into a single computer's RAM at once. This is a situation where the Pangeo ecosystem can help you a lot. Xarray offers the possibility to work lazily on data __chunks__, which means pieces of an entire dataset. By reading a dataset in __chunks__ we can process our data piece by piece on a single computer and even on a distributed computing cluster using Dask (Cloud or HPC for instance).\n",
    "\n",
    "How we will process these 'chunks' in a parallel environment will be discussed in [dask_introduction](./dask_introduction.ipynb). The concept of __chunk__ will be explained here.\n",
    "\n",
    "When we process our data piece by piece, it's easier to have our input or ouput data also saved in __chunks__. [Zarr](https://zarr.readthedocs.io/en/stable/) is the reference library in the Pangeo ecosystem to save our Xarray multidimentional datasets in __chunks__.\n",
    "\n",
    "[Zarr](https://zarr.readthedocs.io/en/stable/) is not the only file format which uses __chunk__. [kerchunk library](https://fsspec.github.io/kerchunk/) for example builds a virtual __chunked__ dataset based on NetCDF files, and optimizes the access and analysis of large datasets.\n",
    "\n",
    "### Data\n",
    "\n",
    "In this workshop, we will be using the [SeasFire Data Cube](https://opensciencedata.esa.int/products/seasfire-cube/collection) published to the EarthCODE Open Science Catalog\n",
    "\n",
    "#### Related publications\n",
    "* *Alonso, Lazaro, Gans, Fabian, Karasante, Ilektra, Ahuja, Akanksha, Prapas, Ioannis, Kondylatos, Spyros, Papoutsis, Ioannis, Panagiotou, Eleannna, Michail, Dimitrios, Cremer, Felix, Weber, Ulrich, & Carvalhais, Nuno. (2022). SeasFire Cube: A Global Dataset for Seasonal Fire Modeling in the Earth System (0.4) [Data set]. Zenodo. @alonso-2024. The same dataset can also be downloaded from Zenodo: https://zenodo.org/records/13834057*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c9c3a0-8d23-454c-8cd5-2186a71e7901",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "http_url = \"https://s3.waw4-1.cloudferro.com/EarthCODE/OSCAssets/seasfire/seasfire_v0.4.zarr/\"\n",
    "\n",
    "ds = xarray.open_dataset(\n",
    "\thttp_url,\n",
    "\tengine='zarr',\n",
    "    chunks={},\n",
    "\tconsolidated=True\n",
    "\t# storage_options = {'token': 'anon'}\n",
    ")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f878112e-a94a-4177-a5b6-c02b53a62931",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size_GB = sum(var.nbytes for var in ds.data_vars.values()) / 1e9  # B to GB\n",
    "print(f\"Total size: {total_size_GB:.2f} GB\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b840af62-c679-40a3-b780-b91135d928be",
   "metadata": {},
   "source": [
    "## What is a __chunk__\n",
    "\n",
    "If you look carefully at our dataset, each Data Variable is a `ndarray` (or `dask.array` if you have a dask client instantiated) with a chunk size of `(966, 180, 360)`. So basically accessing one data variable would load arrays of dimensions `(966, 180, 360)` into the computer's RAM. You can see this information and more details by clicking the icon as indicated in the image below.\n",
    "\n",
    "![Dask.array](../static/datasize.png)\n",
    "\n",
    "When you need to analyze large files, a computer's memory may not be sufficient anymore.\n",
    "\n",
    "This is where understanding and using chunking correctly comes into play.\n",
    "\n",
    "__Chunking__ is splitting a dataset into small pieces. \n",
    "\n",
    "Original dataset is in one piece,  \n",
    "![Dask.array](../static/notchunked.png)\n",
    "\n",
    "and we split it into several smaller pieces.  \n",
    "![Dask.array](../static/chunked.png)\n",
    "\n",
    "We split it into pieces so that we can process our data block by block or __chunk__ by __chunk__.\n",
    "\n",
    "In our case, the data is already chunked, in a cloud-native format ready for analysis and usage - in **zarr format**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f79246d-cb25-4473-bf6d-db8f7aff8517",
   "metadata": {},
   "source": [
    "# Zarr\n",
    "\n",
    "Zarr's main characteristics are the following:\n",
    "\n",
    "- Every chunk of a Zarr dataset is stored as a single file\n",
    "- Each Data array in a Zarr dataset has a two unique files containing metadata:\n",
    "  - .zattrs for dataset or dataarray general metadatas\n",
    "  - .zarray indicating how the dataarray is chunked, and where to find them on disk or other storage.\n",
    "  \n",
    "Zarr can be considered as an Analysis Ready, cloud optimized data (ARCO) file format!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edc-lps25-edc-lps25-edc_pangeo_env",
   "language": "python",
   "name": "conda-env-edc-lps25-edc-lps25-edc_pangeo_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
